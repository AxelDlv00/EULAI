{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019189c9",
   "metadata": {},
   "source": [
    "# Analysis of the policies & classification\n",
    "\n",
    "*This notebook uses Gemini AI to analyze privacy policy documents and extract important clauses classified as BLOCKER, BAD, NEUTRAL, or GOOD according to ToS;DR standards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b316b8",
   "metadata": {},
   "source": [
    "## 1. Load Libraries\n",
    "\n",
    "*Import required packages for async processing, AI model interaction, environment variables, and rich console visualization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e65ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "from typing import List, Dict\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.progress import (\n",
    "    Progress, SpinnerColumn, BarColumn, TextColumn, \n",
    "    TimeRemainingColumn, MofNCompleteColumn\n",
    ")\n",
    "from rich.panel import Panel\n",
    "from rich.rule import Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e842c",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "*Define file paths, AI model settings, processing limits, and initialize the Google Gemini client.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e945e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('../..')\n",
    "DATA_DIR = ROOT / \"data-generated\" / \"TOSDR\"\n",
    "MARKDOWN_OUTPUT = DATA_DIR / \"policies_md.jsonl\"\n",
    "HIGHLIGHTS_OUTPUT = DATA_DIR / \"gemini_policies_md.jsonl\"\n",
    "ENV_FILE = ROOT / \".env\"\n",
    "model_name = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "# Qwen3-0.6B optimization: Blocks ~4500 chars (approx 1024 tokens)\n",
    "MAX_CHARS_PER_BLOCK = 4500 \n",
    "MAX_TO_PROCESS = 1000\n",
    "CONCURRENCY_LIMIT = 5\n",
    "\n",
    "console = Console()\n",
    "load_dotenv(ENV_FILE)\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_AI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921397f",
   "metadata": {},
   "source": [
    "## 3. AI System Prompt\n",
    "\n",
    "*Define the prompt that instructs Gemini on how to analyze documents and classify clauses according to ToS;DR standards.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d8ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert legal auditor for ToS;dr. Your mission is to parse legal document blocks and extract ONLY the clauses that impact a user's rights, privacy, or safety.\n",
    "\n",
    "CRITICAL PHILOSOPHY:\n",
    "- DISTINGUISH BOILERPLATE FROM PROTECTIONS: Generic greetings or index lists are \"NO_CLAUSES_FOUND\". However, explicit promises (encryption, support, data limits) are valid points.\n",
    "- BE SELECTIVE BUT COMPLETE: If a block contains 3 risks and 1 protection, extract all 4. \n",
    "- IGNORE PURE STRUCTURE: Skip table of contents, contact addresses (unless for rights exercise), and section headers without text.\n",
    "\n",
    "STRICT CONSTRAINTS:\n",
    "- ZERO EXTERNAL KNOWLEDGE: Do not use anything you know about this company from your training data. \n",
    "- TEXTUAL EVIDENCE ONLY: If a clause is not explicitly written in the provided segment, it does NOT exist.\n",
    "- NO HALLUCINATIONS: Do not assume or infer policies. If the text doesn't mention \"Arbitration\", do not list an arbitration clause even if you know the company uses one.\n",
    "- ANALYZE AS A BLIND DOCUMENT: Treat this segment as if you have never heard of the company before.\n",
    "\n",
    "CLASSIFICATION:\n",
    "- [GOOD]: Positive for user rights or security (e.g., encryption, clear notification periods, data deletion).\n",
    "- [NEUTRAL]: Important facts for transparency (e.g., jurisdiction, specific age limits).\n",
    "- [BAD]: Negative practices or risks (e.g., arbitration, tracking).\n",
    "- [BLOCKER]: Critical dangers (e.g., data selling, private message access).\n",
    "\n",
    "FEW-SHOT EXAMPLES:\n",
    "\n",
    "Segment: \n",
    "\"1. Introduction\n",
    "2. Account Registration\n",
    "3. Fees and Payments\n",
    "4. Termination Policy\n",
    "For more information, visit our help center or contact us at support@example.com.\"\n",
    "Output: NO_CLAUSES_FOUND\n",
    "\n",
    "Segment: \n",
    "\"Welcome to our platform. We provide cloud storage features. We use industry-standard AES-256 encryption to protect your data at rest. Our customer support team is available 24/7 via live chat to assist with any security concerns.\"\n",
    "Output:\n",
    "- [GOOD] : Strong Encryption : The service uses industry-standard AES-256 encryption to protect stored data.\n",
    "- [GOOD] : 24/7 Support : Users have constant access to support for technical or security issues.\n",
    "\n",
    "Segment:\n",
    "\"## 5. Intellectual Property\n",
    "You retain ownership of your photos. However, you grant us a worldwide, perpetual license to use, reproduce, and distribute your content. We may also use your username in commercial ads without compensation.\"\n",
    "Output:\n",
    "- [BLOCKER] : Perpetual Content License : The service takes an irrevocable and perpetual license to use all your content.\n",
    "- [BAD] : Commercial use of Identity : The service can use your username for advertising without payment.\n",
    "\n",
    "Segment:\n",
    "\"We store your data for 30 days after account deletion. Any disputes will be resolved in the courts of Paris, France. We do not track your location when the app is closed.\"\n",
    "Output:\n",
    "- [NEUTRAL] : Data Retention Period : Personal data is kept for 30 days after the account is closed.\n",
    "- [NEUTRAL] : Jurisdiction : Disputes are handled specifically in Paris, France.\n",
    "- [GOOD] : No Background Tracking : The app explicitly stops location tracking when not in active use.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "- [LABEL] : SHORT TITLE : Concise explanation.\n",
    "\n",
    "If the segment contains only structure or irrelevant text, return: NO_CLAUSES_FOUND\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30010c79",
   "metadata": {},
   "source": [
    "## 4. Processing Functions\n",
    "\n",
    "*Functions for parsing AI output, extracting highlights, and building the dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a475ba8",
   "metadata": {},
   "source": [
    "### 4.1 Segmentation of markdowns\n",
    "\n",
    "*Segment a markdown into blocs, it uses a robust approach to ensure each bloc remains under the max context*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bf0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hierarchical(text: str, max_chars: int = MAX_CHARS_PER_BLOCK) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively splits markdown into independent blocks following logical priority:\n",
    "    H1 (#) -> H2 (##) -> H3 (###) -> Paragraphs (\\n\\n) -> Lines (\\n)\n",
    "    \"\"\"\n",
    "    def split_recursive(content: str, separators: List[str]) -> List[str]:\n",
    "        if len(content) <= max_chars or not separators:\n",
    "            return [content]\n",
    "        \n",
    "        current_sep = separators[0]\n",
    "        remaining_seps = separators[1:]\n",
    "        \n",
    "        # Split using lookahead to keep headers with their content\n",
    "        parts = re.split(current_sep, content)\n",
    "        \n",
    "        final_parts = []\n",
    "        current_buffer = \"\"\n",
    "        \n",
    "        for part in parts:\n",
    "            if not part: continue\n",
    "            \n",
    "            if len(part) > max_chars:\n",
    "                if current_buffer:\n",
    "                    final_parts.append(current_buffer.strip())\n",
    "                    current_buffer = \"\"\n",
    "                final_parts.extend(split_recursive(part, remaining_seps))\n",
    "            elif len(current_buffer) + len(part) > max_chars:\n",
    "                if current_buffer:\n",
    "                    final_parts.append(current_buffer.strip())\n",
    "                current_buffer = part\n",
    "            else:\n",
    "                current_buffer += part\n",
    "        \n",
    "        if current_buffer:\n",
    "            final_parts.append(current_buffer.strip())\n",
    "            \n",
    "        return final_parts\n",
    "\n",
    "    # Ordered priority of separators\n",
    "    separators = [\n",
    "        r'\\n(?=#\\s)',    # H1\n",
    "        r'\\n(?=##\\s)',   # H2\n",
    "        r'\\n(?=###\\s)',  # H3\n",
    "        r'\\n\\n',         # Paragraphs\n",
    "        r'\\n'            # Lines\n",
    "    ]\n",
    "    \n",
    "    return [c for c in split_recursive(text, separators) if len(c.strip()) > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c71d34",
   "metadata": {},
   "source": [
    "### 4.2 Output Parser\n",
    "\n",
    "*Parses the AI-generated text to extract structured highlights with labels (BLOCKER, BAD, GOOD, NEUTRAL).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301467a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generative_output(text: str) -> List[Dict]:\n",
    "    results = []\n",
    "    # Pattern handles optional dash and flexible spacing\n",
    "    pattern = re.compile(r\"^[-]?\\s*\\[(BAD|GOOD|NEUTRAL|BLOCKER)\\]\\s*:\\s*([^:]+):\\s*(.+)$\", re.MULTILINE)\n",
    "    \n",
    "    for match in pattern.finditer(text):\n",
    "        results.append({\n",
    "            \"label\": match.group(1).upper(),\n",
    "            \"title\": match.group(2).strip(),\n",
    "            \"explanation\": match.group(3).strip()\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e014f",
   "metadata": {},
   "source": [
    "### 4.3 AI Extraction Worker\n",
    "\n",
    "*Sends document to Gemini AI for analysis and returns parsed highlights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58c027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_highlights(document_item, semaphore):\n",
    "    \"\"\"Segments the document and returns a list of independent block objects.\"\"\"\n",
    "    markdown_text = document_item.get('policy', '')\n",
    "    if not markdown_text: return []\n",
    "\n",
    "    chunks = segment_hierarchical(markdown_text)\n",
    "    blocks_results = []\n",
    "\n",
    "    async with semaphore:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        system_instruction=SYSTEM_PROMPT,\n",
    "                        temperature=0.0,\n",
    "                    ),\n",
    "                    contents=f\"STRICTLY ANALYZE THIS TEXT ONLY:\\n\\n{chunk}\"\n",
    "                )\n",
    "                \n",
    "                generated_text = response.text.strip() if response.text else \"\"\n",
    "                \n",
    "                final_output = \"\" if \"NO_CLAUSES_FOUND\" in generated_text else generated_text\n",
    "                \n",
    "                c_hash = hashlib.md5(chunk.encode('utf-8')).hexdigest()[:8]\n",
    "                block_id = f\"{document_item['service_id']}_{i}_{c_hash}\"\n",
    "\n",
    "                blocks_results.append({\n",
    "                    \"id\": block_id,\n",
    "                    \"original_service_id\": document_item[\"service_id\"],\n",
    "                    \"service_name\": document_item[\"service_name\"],\n",
    "                    \"url\": document_item[\"url\"],\n",
    "                    \"input\": chunk,\n",
    "                    \"output\": final_output\n",
    "                })\n",
    "                \n",
    "                await asyncio.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                console.print(f\"[red]Error on chunk {i} of {document_item['service_name']}: {e}[/red]\")\n",
    "                continue\n",
    "                \n",
    "    return blocks_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f9d54",
   "metadata": {},
   "source": [
    "### 4.4 Main Dataset Builder\n",
    "\n",
    "*Coordinates the full pipeline: loading markdown files, deduplicating content, processing with AI, and saving results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5ea2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_dataset_builder():\n",
    "    if not MARKDOWN_OUTPUT.exists():\n",
    "        console.print(\"[red]Source file missing.[/red]\")\n",
    "        return\n",
    "\n",
    "    already_processed_ids = set()\n",
    "    if HIGHLIGHTS_OUTPUT.exists():\n",
    "        with open(HIGHLIGHTS_OUTPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try: \n",
    "                    already_processed_ids.add(json.loads(line).get(\"original_service_id\"))\n",
    "                except: continue\n",
    "\n",
    "    unprocessed_documents = []\n",
    "    with open(MARKDOWN_OUTPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                doc_data = json.loads(line)\n",
    "                if doc_data.get(\"service_id\") not in already_processed_ids:\n",
    "                    unprocessed_documents.append(doc_data)\n",
    "            except: continue\n",
    "\n",
    "    if not unprocessed_documents:\n",
    "        console.print(\"[bold green]âœ” Everything processed![/bold green]\")\n",
    "        return\n",
    "\n",
    "    unprocessed_documents = unprocessed_documents[:MAX_TO_PROCESS]\n",
    "    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "    \n",
    "    with Progress(\n",
    "        SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\"),\n",
    "        BarColumn(), MofNCompleteColumn(), TimeRemainingColumn(), console=console,\n",
    "    ) as progress:\n",
    "        task = progress.add_task(\"[cyan]Processing documents...\", total=len(unprocessed_documents))\n",
    "        \n",
    "        with open(HIGHLIGHTS_OUTPUT, \"a\", encoding=\"utf-8\") as output_file:\n",
    "            for document in unprocessed_documents:\n",
    "                progress.update(task, description=f\"[cyan]Analyzing {document['service_name']}...\")\n",
    "                \n",
    "                # Get the list of processed blocks\n",
    "                processed_blocks = await extract_highlights(document, semaphore)\n",
    "                \n",
    "                for block in processed_blocks:\n",
    "                    output_file.write(json.dumps(block, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                output_file.flush()\n",
    "                progress.advance(task)\n",
    "\n",
    "    console.print(f\"\\n[bold green]Complete![/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac848c3",
   "metadata": {},
   "source": [
    "## 5. Run Highlight Extraction\n",
    "\n",
    "*Execute the main analysis pipeline to generate highlights for all documents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552a6f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada2ddcb912c482dac36b7ae14a4ede9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mComplete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await main_dataset_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ece1f0",
   "metadata": {},
   "source": [
    "## 6. Train/Validation Split\n",
    "\n",
    "*Create training and test datasets by deduplicating documents and splitting into train/test sets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5313bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks available: 22005\n",
      "Saved: ../../data-generated/EULAI/qwen_train.jsonl (20904 blocks)\n",
      "Saved: ../../data-generated/EULAI/qwen_test.jsonl (1101 blocks)\n"
     ]
    }
   ],
   "source": [
    "def run_split():\n",
    "    if not HIGHLIGHTS_OUTPUT.exists():\n",
    "        print(\"No analyzed data found.\")\n",
    "        return\n",
    "\n",
    "    block_dataset = []\n",
    "    with open(HIGHLIGHTS_OUTPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                block_dataset.append(json.loads(line))\n",
    "            except: continue\n",
    "\n",
    "    print(f\"Total blocks available: {len(block_dataset)}\")\n",
    "    \n",
    "    train_set, test_set = train_test_split(block_dataset, test_size=0.05, random_state=42)\n",
    "    \n",
    "    output_dir = ROOT / \"data-generated\" / \"EULAI\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for name, samples in [(\"train\", train_set), (\"test\", test_set)]:\n",
    "        output_path = output_dir / f\"qwen_{name}.jsonl\"\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for s in samples:\n",
    "                f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"Saved: {output_path} ({len(samples)} blocks)\")\n",
    "\n",
    "run_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_MINES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
