{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3619cf",
   "metadata": {},
   "source": [
    "# Retrieve ToS;DR Data\n",
    "\n",
    "*This notebook retrieves service information from the ToS;DR API, including service names and associated privacy policy documents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818259e1",
   "metadata": {},
   "source": [
    "## 1. Load Libraries\n",
    "\n",
    "*Import required packages for HTTP requests, parallel processing, and progress visualization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5935697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.progress import (\n",
    "    Progress, SpinnerColumn, BarColumn, TextColumn, \n",
    "    TimeRemainingColumn, MofNCompleteColumn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb5c85",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "*Define API endpoints, user agents for requests, file paths, and thread-safe locks for concurrent file writes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa188f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "API_BASE = \"https://api.tosdr.org\"\n",
    "\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "]\n",
    "\n",
    "ROOT = Path('../..')\n",
    "DATA_DIR = ROOT / \"data-generated\" / \"TOSDR\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_FILE = DATA_DIR / \"tosdr_data.jsonl\"\n",
    "ID_FILE = DATA_DIR / \"tosdr_ids.txt\"\n",
    "\n",
    "# Thread-safe lock for file writing\n",
    "file_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edd013",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "*Utility functions for making safe HTTP requests with retry logic and rate limit handling.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713bedb",
   "metadata": {},
   "source": [
    "### 3.1 HTTP Request Handler\n",
    "\n",
    "*Performs GET requests with automatic retry on failures and exponential backoff for rate limits.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get(url, retries=3, backoff=5):\n",
    "    \"\"\"Performs a simple and fast GET request with retry logic.\"\"\"\n",
    "    headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            time.sleep(random.uniform(0.1, 0.3)) \n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            \n",
    "            if resp.status_code == 429:\n",
    "                wait_time = backoff * (i + 1)\n",
    "                if wait_time > 10:\n",
    "                    console.print(f\"[red]⚡ Rate limit hit, pausing ({wait_time}s)...[/red]\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "                \n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9b6fa",
   "metadata": {},
   "source": [
    "### 3.2 Service Index Fetcher\n",
    "\n",
    "*Retrieves the complete list of service IDs by paginating through the ToS;DR API until no more results are found.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_service_index():\n",
    "    \"\"\"Quickly retrieves all service IDs using V3 pagination.\"\"\"\n",
    "    valid_ids = []\n",
    "    current_page = 1\n",
    "    consecutive_empty_pages = 0\n",
    "    \n",
    "    console.print(\"[cyan]Fetching service list...[/cyan]\")\n",
    "    \n",
    "    with console.status(\"Scanning index...\") as status:\n",
    "        while True:\n",
    "            resp = safe_get(f'{API_BASE}/service/v3/?page={current_page}')\n",
    "            if not resp: break\n",
    "            \n",
    "            try:\n",
    "                data = resp.json()\n",
    "                if isinstance(data, list):\n",
    "                    services = data\n",
    "                else:\n",
    "                    services = data.get('parameters', {}).get('services', [])\n",
    "                    if not services: services = data.get('services', [])\n",
    "                \n",
    "                if not services:\n",
    "                    consecutive_empty_pages += 1\n",
    "                    if consecutive_empty_pages >= 3: break\n",
    "                else:\n",
    "                    consecutive_empty_pages = 0\n",
    "                    for service in services:\n",
    "                        if isinstance(service, dict) and service.get('id'):\n",
    "                            valid_ids.append(service['id'])\n",
    "                \n",
    "                status.update(f\"Page {current_page} - {len(valid_ids)} services found\")\n",
    "                current_page += 1\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "    unique_ids = sorted(list(set(valid_ids)))\n",
    "    console.print(f\"[green]✔ Index complete: {len(unique_ids)} unique services.[/green]\")\n",
    "    return unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269069e8",
   "metadata": {},
   "source": [
    "## 4. Data Extraction\n",
    "\n",
    "*Functions to fetch detailed service information and handle the API's inconsistent JSON response structures.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f81c6",
   "metadata": {},
   "source": [
    "### 4.1 Service Detail Worker\n",
    "\n",
    "*Worker function that fetches individual service details and normalizes the inconsistent API response formats.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_service_worker(session, service_id):\n",
    "    \"\"\"\n",
    "    Worker capable of handling inconsistent JSON structures from the ToS;DR API.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/service/v3/?id={service_id}\"\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "            \n",
    "            resp = session.get(url, timeout=15)\n",
    "            \n",
    "            if resp.status_code == 429:\n",
    "                if attempt == 2: return {\"error\": \"Rate Limited (429)\", \"id\": service_id}\n",
    "                time.sleep(5 * (attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            if resp.status_code != 200:\n",
    "                return {\"error\": f\"HTTP {resp.status_code}\", \"id\": service_id, \"skippable\": True}\n",
    "\n",
    "            try:\n",
    "                raw_data = resp.json()\n",
    "            except json.JSONDecodeError:\n",
    "                return {\"error\": \"Invalid JSON\", \"id\": service_id}\n",
    "\n",
    "            service_info = None\n",
    "\n",
    "            if isinstance(raw_data, dict) and raw_data.get(\"name\") and raw_data.get(\"documents\") is not None:\n",
    "                service_info = raw_data\n",
    "            \n",
    "            elif \"parameters\" in raw_data:\n",
    "                params = raw_data[\"parameters\"]\n",
    "                if isinstance(params, dict):\n",
    "                    if \"name\" in params:\n",
    "                        service_info = params\n",
    "                    elif \"services\" in params and isinstance(params[\"services\"], list) and len(params[\"services\"]) > 0:\n",
    "                        service_info = params[\"services\"][0]\n",
    "\n",
    "            elif \"services\" in raw_data and isinstance(raw_data[\"services\"], list) and len(raw_data[\"services\"]) > 0:\n",
    "                service_info = raw_data[\"services\"][0]\n",
    "\n",
    "            if not service_info:\n",
    "                return {\"error\": \"Empty JSON or unknown structure\", \"id\": service_id, \"skippable\": True}\n",
    "\n",
    "            if not service_info.get(\"name\"):\n",
    "                 return {\"error\": \"Service name missing\", \"id\": service_id, \"skippable\": True}\n",
    "\n",
    "            documents = []\n",
    "            raw_documents = service_info.get(\"documents\", [])\n",
    "            if raw_documents: \n",
    "                for doc in raw_documents:\n",
    "                    if doc.get(\"url\"):\n",
    "                        documents.append({\n",
    "                            \"name\": doc.get(\"name\"), \n",
    "                            \"url\": doc.get(\"url\")\n",
    "                        })\n",
    "\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"data\": {\n",
    "                    \"service_id\": service_id,\n",
    "                    \"name\": service_info.get(\"name\"),\n",
    "                    \"documents\": documents\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == 2: return {\"error\": f\"Exception: {str(e)}\", \"id\": service_id}\n",
    "            time.sleep(1)\n",
    "\n",
    "    return {\"error\": \"Max retries\", \"id\": service_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e28625",
   "metadata": {},
   "source": [
    "### 4.2 Main Execution Pipeline\n",
    "\n",
    "*Coordinates the full extraction process: loading IDs, checking for already processed items, and running parallel workers with progress tracking.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    console.print(f\"[bold cyan]Data directory:[/bold cyan] {DATA_DIR}\")\n",
    "    \n",
    "    if not ID_FILE.exists():\n",
    "        console.print(f\"[red]IDs file missing: {ID_FILE}[/red]\")\n",
    "        return\n",
    "\n",
    "    with open(ID_FILE, \"r\") as f:\n",
    "        all_service_ids = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    console.print(f\"IDs loaded: {len(all_service_ids)}\")\n",
    "\n",
    "    processed_ids = set()\n",
    "    if DATA_FILE.exists():\n",
    "        console.print(\"[yellow]Reading checkpoint...[/yellow]\")\n",
    "        with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    if \"service_id\" in item:\n",
    "                        processed_ids.add(str(item[\"service_id\"]))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    remaining_ids = [service_id for service_id in all_service_ids if str(service_id) not in processed_ids]\n",
    "    \n",
    "    console.print(f\"[green]Already processed: {len(processed_ids)}[/green]\")\n",
    "    console.print(f\"[bold blue]Remaining to process: {len(remaining_ids)}[/bold blue]\")\n",
    "\n",
    "    if not remaining_ids:\n",
    "        console.print(\"[green]Everything is already up to date![/green]\")\n",
    "        return\n",
    "\n",
    "    f_out = open(DATA_FILE, \"a\", encoding=\"utf-8\", buffering=1)\n",
    "\n",
    "    try:\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            MofNCompleteColumn(),\n",
    "            TimeRemainingColumn(),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "            \n",
    "            extraction_task = progress.add_task(\"Extracting...\", total=len(remaining_ids))\n",
    "            \n",
    "            with requests.Session() as session:\n",
    "                session.headers.update({'User-Agent': random.choice(USER_AGENTS)})\n",
    "                \n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                    future_to_service_id = {\n",
    "                        executor.submit(fetch_service_worker, session, service_id): service_id \n",
    "                        for service_id in remaining_ids\n",
    "                    }\n",
    "                    \n",
    "                    for future in concurrent.futures.as_completed(future_to_service_id):\n",
    "                        current_service_id = future_to_service_id[future]\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            \n",
    "                            if result and result.get(\"success\"):\n",
    "                                data_str = json.dumps(result[\"data\"], ensure_ascii=False)\n",
    "                                with file_lock:\n",
    "                                    f_out.write(data_str + \"\\n\")\n",
    "                                    f_out.flush()\n",
    "                            else:\n",
    "                                if result and not result.get(\"skippable\"):\n",
    "                                    error_msg = result.get(\"error\", \"Unknown\")\n",
    "                                    progress.console.print(f\"[red]✘ ID {current_service_id}: {error_msg}[/red]\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            progress.console.print(f\"[red]Critical error {current_service_id}: {e}[/red]\")\n",
    "                        \n",
    "                        progress.update(extraction_task, advance=1)\n",
    "    \n",
    "    finally:\n",
    "        f_out.close()\n",
    "        console.print(f\"[bold green]✔ Complete! File: {DATA_FILE}[/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aab12",
   "metadata": {},
   "source": [
    "## 5. Run Extraction\n",
    "\n",
    "*Execute the main pipeline to fetch and save all service data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb904fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_MINES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
